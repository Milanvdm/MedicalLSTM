\graphicspath{ {FutureWork/Images/} }


\chapter{Future Work}
\label{cha:futureWork}

\section{Introduction}
In this chapter 

Normalization
Distributed
RNN


\section{Generalization}

In our word2vec approach we applied generalization on the medical states. This was needed to retrieve more general n-grams. For this generalization, we divided some attributes into specific intervals.

Instead of dividing some attributes into specific intervals, we could apply normalization to it. Based on the distribution of the data, we can make more sensible intervals and assign them to the attributes.


\section{Distributed Word2Vec}

Word2vec can be made distributed as the underlying idea is quite simplistic, it counts occurrences of n-grams. Counting occurrences based on labels, is a well known problem and is often solved by MapReduce algorithms. 


\section{Patient Classification}

As mentioned in section \ref{sec:word2vec}, a trained 2-layer neural network can be placed before another neural network and function as a lookup table. In this section, we discuss a possible neural network which allows us to further investigate the effectiveness of our word2vec approach to classify patients. More concrete: we should check if a better accuracy is acquired with the lookup table in front of the neural network or without. 

\subsection{Problem Definition}

The medical history of a patient is seen as a time series with as datapoints an EHR. Based on the time series, we want to classify it into different disease trajectories. A patient who is classified into a specific disease trajectory, can be treated more specifically. \\

The medical data of multiple patients is a $3$ dimensional tensor, see figure \ref{fig:rnnData}. This data structure is the input structure for a neural network.

\begin{figure}[H]
	\centering
	\includegraphics[width=8cm]{rnnData.png}
	\caption{Overview of the data structure for medical data with a time aspect.}
	\label{fig:rnnData}
\end{figure} 

Medical data has some problems which we will discuss.\\
It often consists of long time periods. This means there could be a long range of dependencies between events. In the context of training neural networks, this can cause a problem known as the vanishing gradient problem. \\
Patients don't have regular intervals in their medical data. The irregular intervals need to be transformed to regular intervals otherwise the time aspect won't be consistent throughout the data. \\
The standardization of the attributes needs to be taken into account. Preferably some sort of normalization should be applied as well. \\
Medical data has a high dimensionality. A lot of parameters need to be taken into account to retrieve accurate results. This causes the well known problem: Curse of Dimensionality. It causes the data to be sparse and therefore, more data is needed. Especially in medical data where outliers are important. 


\subsection{Approach}

Here we describe our approaches for the problems mentioned in the previous section. \\
We solve the vanishing gradient problem with a special for of recurrent neural network, see section \ref{sec:nn}. \\
By applying our word2vec approach, the input is projected on another vector space using a lookup table. This vector space causes normalization. The standardization is also done in our word2vec approach. \\
As we mentioned, the Curse of Dimensionality causes the need for more data. The neural network in section \ref{sec:nn} often handles high dimensional data. In a sense, because it keeps track of the time aspect of the data, it uses the data more thoroughly and thus has a better method to handle the high dimensionality.

\subsubsection{Padding and Masking}
The transformation of the irregular interval to a regular one, is done with padding and masking. \\

If we don't use any masking and padding, our data can only be of equal length time series and at each time step a classification. Our data consists of several inputs, the different time steps of a time series, and has one output associated with it, namely the classification of the time series. \\

The method of padding is simply by adding empty events (ex. zeros) to the shorter time series until all examples are of equal length for both input and output. \\
Using padding, changes the data quite drastically and would case problems during the training because of that. For this problem we use the method of masking. With masking, we have two additional arrays which contain the information about whether an input or output is padding or not. See figure \ref{fig:masking}, picture $2$ on how the masking is done for a many to one case.

\begin{figure}[H]
	\centering
	\includegraphics[width=8cm]{masking.png}
	\caption{Multiple masking methods.}
	\label{fig:masking}
\end{figure} 



\subsection{Neural Network}
\label{sec:nn}

We mentioned the reasons on why we choose a Long Short Term Memory (LSTM) approach as a recurrent neural network solution. In this section we explain in more detail why LSTM handles the vanishing problem and long-term dependencies. \\

First we shortly repeat the structure of a neural network in figure \ref{fig:nnStructure}. We see the input, different layers with their perceptrons, and $\sigma$ as the activation function. 

\begin{figure}[H]
	\centering
	\includegraphics[width=8cm]{nnStructure.png}
	\caption{A general structure of a neural network.}
	\label{fig:nnStructure}
\end{figure} 

\subsubsection{Recurrent Neural Network}

A standard neural network don't have any persistence. They will classify their input but when they get a stream of inputs (ex. speech), they will classify each word independently of each other and without any regards of the previous words. A recurrent neural network (RNN) addresses this problem by introducing networks with loops. This way, the output of a previous input has effect on the next input. In figure \ref{fig:rnnLoops}, we transform those loops into multiple copies of the same network which makes it easier to reason about.

\begin{figure}[H]
	\centering
	\includegraphics[width=8cm]{rnnLoops.png}
	\caption{An unrolled recurrent neural network.}
	\label{fig:rnnLoops}
\end{figure} 

The problem with RNNs is mainly that they have troubles learning long-term dependencies which is often essential in time series.


\subsubsection{Long Short Term Memory}

A LSMT network is a specific RNN which is capable of learning long-term dependencies. We will explain the difference with a standard RNN and why a LSTM can learn these long-term dependencies. \\

A recurrent network is, as we said, a chain of connected neural networks. Those networks can have a simple structure as a single $tanh$ layer, see figure \ref{fig:rnnTanh}.

\begin{figure}[H]
	\centering
	\includegraphics[width=8cm]{rnnTanh.png}
	\caption{An unrolled recurrent neural network with a single tanh layer.}
	\label{fig:rnnTanh}
\end{figure} 

It is important to see the difference with a LSTM. The repeating network doesn't have a single neural network layer, but has $4$ layers which each fulfills a specific goal, see figure \ref{fig:LSTMChain}.

\begin{figure}[H]
	\centering
	\includegraphics[width=8cm]{LSTMChain.png}
	\caption{An unrolled LSTM network where each network has 4 layers.}
	\label{fig:LSTMChain}
\end{figure} 

The main idea behind LSTM is that each repeated network has its own cell state. It functions as a  memory which can be updated with each new input. On figure \ref{fig:LSTMConveyor}, you can see the cell state $C$ through time. It can be compared with a conveyor belt which interacts with the input at certain gates. This way the state is updated throughout several inputs.

\begin{figure}[H]
	\centering
	\includegraphics[width=8cm]{LSTMConveyor.png}
	\caption{Representation of the cell state for a LSTM network.}
	\label{fig:LSTMConveyor}
\end{figure} 

In the following figures, we show the different gates and their functions in changing the cell state depending on the input and the output of the previous network. Next to each figure the formulas are shown on how the cell state is updated. There should be no surprises as they are not much different than the standard formulas of neural networks. \\

We start with the forget gate layer of a LSTM. Based on $x_t$ and $h_{t-1}$, it outputs a number between $0$ and $1$ for each number in the cell state $C_{t-1}$. This is shown in figure \ref{fig:LSTM1}.

\begin{figure}[H]
	\centering
	\includegraphics[width=8cm]{LSTM1.png}
	\caption{The forget layer of a LSTM network.}
	\label{fig:LSTM1}
\end{figure} 

Next we look to the input gate layer. This gate decides which values will be updated in the cell state and outputs those in $i_t$. It is then combined with the vector $\widetilde{C}_t$, which contains the new candidate values based on the input $x_t$ and $h_{t-1}$. This is shown in figure \ref{fig:LSTM2}.

\begin{figure}[H]
	\centering
	\includegraphics[width=8cm]{LSTM2.png}
	\caption{The input layer of a LSTM network.}
	\label{fig:LSTM2}
\end{figure} 

We can now combine the previous results and adjust the cell state. We multiply the old state with $f_t$ so we forget the needed elements. Then we add $i_t*\widetilde{C}_t$ which are the new candidate values multiplied by the amount on how much we want to update each state value. See figure \ref{fig:LSTM3}.

\begin{figure}[H]
	\centering
	\includegraphics[width=8cm]{LSTM3.png}
	\caption{Update process of the cell state of a LSTM network.}
	\label{fig:LSTM3}
\end{figure} 

Finally, we need to output $h_t$ to the next network. This is based on the input and the cell state $C_t$. See figure \ref{fig:LSTM4}.

\begin{figure}[H]
	\centering
	\includegraphics[width=8cm]{LSTM4.png}
	\caption{Decide the output of a LSTM network.}
	\label{fig:LSTM4}
\end{figure} 


\subsubsection{Variants Long Short Term Memory}

In [LSTM: SEARCH SPACE ODESEY], research is done between different variant of LSTM networks. It was concluded that the forget gate and the activation function is the most important. Other variants don't have a large influence and mainly add a lot of extra complexity.


http://www-dsi.ing.unifi.it/\~paolo/ps/tnn-94-gradient.pdf


http://karpathy.github.io/2015/05/21/rnn-effectiveness/

http://deeplearning4j.org/usingrnns

Supervised Sequence Labelling with Recurrent Neural
Networks

Phenotyping of Clinical Time Series with LSTM
Recurrent Neural Networks

IMEC Technical talk by Jaak Simm

Missing covariate data in medical research: To impute
is better than to ignore


Recurrent Neural Networks for Missing or
Asynchronous Data

Development of a Database of Health Insurance
Claims: Standardization of Disease Classifications and
Anonymous Record Linkage

Machine Learning Based Missing Value Imputation
Method for Clinical Dataset

Modeling Temporal Dependencies in High-
Dimensional Sequences: Application to Polyphonic
Music Generation and Transcription

Long short-term memory neural network for traffic
speed prediction using remote microwave sensor data


Noisy Time Series Prediction using a Recurrent Neural
Network and Grammatical Inference

Neural Networks for Time Series Processing
Constructing a Non-Linear Model with Neural
Networks for Workload Characterization



http://colah.github.io/posts/2015-08-Understanding-
LSTMs

A Critical Review of Recurrent Neural Networks
for Sequence Learning

Bidirectional Recurrent Neural Networks
Optimization of Hidden Markov Models and Neural
Networks


LONG SHORT-TERM MEMORY

LSTM: A Search Space Odyssey

\section{Conclusion}
The final section of the chapter gives an overview of the important results
of this chapter. This implies that the introductory chapter and the
concluding chapter don't need a conclusion.



%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
