\graphicspath{ {Implementation/Images/} }


\chapter{Model Building}
\label{cha:implementation}

\section{Introduction}
In this chapter 

DiseaseMapping (generalization)
OSIM
Clusters
TensorFlow
DL4J

\section{OSIM2 Dataset}

To validate our approaches mentioned in section \ref{sec:gw2v}, we rely on the Observational Medical Dataset Simulator Generation 2 (OSIM2) dataset \cite{OSIM:online}. This dataset was generated by the Observational Medical Outcomes Partnership (OMOP) as a golden standard to be able to reproduce studies about the effects of drug treatments. It contains around $10$ million of hypothetical patients based on Thomson Reuters MarketScan Lab Database (MSLR). MSLR contains administrative claims between 2003 and 2009 from a privately-insured population. There are $16$ different OSIM2 datasets where each of them is injected with different kinds of signals. We rely on the "OSIM2\_10M\_MSLR\_MEDDRA\_14". \\

The OSIM2 dataset contains multiple database tables which are dumped as comma-separated values (csv) files. The files contain information about the patients' demographics, diagnoses, drug treatments, timestamps, hospital visits, \ldots \\
To make it easier to work with this dataset, we joined the multiple files into one file with on each row an event of a patient containing all relevant information. The relevant information which is kept is: birth year, gender, condition type, condition (MedDRA coding), time difference since previous diagnosis, and season (summer, fall, winter, spring). We ignored drug treatments and hospital visits as those are too complex to categorize. Thus, one EHR event (or vector $m$) is $6$ dimensional, see table \ref{tab:general} for an example. \\	

We will compare our model with the clusters found in Anders Boeck Jensen et. \cite{Brunak:article} (see section \ref{sec:brunak}). Although these clusters are not a golden standard, it is the only study to which we can compare our model. It provides an initial idea on how well our model estimates disease relations. In section \ref{sec:PatientClassification}, we introduce a validation approach which can be explored in the future. \\
Before we can compare our model to the clusters found in the Danish paper, we have to apply several estimations and methods which are described in section \ref{sec:convertionClusters}.


\section{Software Ecosystem}

To build our model, we look for a software library which provides us with the following tools:

\begin{itemize}

\item Word2Vec implementation
\item Highly customizable as we want to adjust internal workings of Word2Vec
\item Built with large datasets in mind (no memory leaks, multi-threading)

\end{itemize}

\subsection{TensorFlow}

TensorFlow is an open-source machine learning software library released at the end of 2015 \cite{tensorflow:article}. It is developed by the Google Brain Team. It puts its focus on neural networks and their applications in language processing and image recognition. \\
It provides a Python interface for efficient C++ code. We found that Tensorflow is not well documented and does not provide the freedom needed to easily rewrite some core features of the Word2Vec implementation, for example manipulating the internal trained lookup table to add new instances based on knn.


\subsection{DeepLearning4Java}

DeepLearning4Java (DL4J) is an open-source machine learning software library released by Skymind \cite{dl4j:article}. It puts its focus on deeplearning and makes it possible to easily extend DL4J's implementation with the user's implementation. \\
It runs on their scientific computing engine ND4J which provides fast matrix operations. DL4J is completely written in Java and provides a lot of freedom to manipulate lookup tables and extend Word2Vec methods to work on abstract objects such as vectors.


\section{Generalized Word2Vec Model Building}
\label{sec:convertionClusters}

As described in section \ref{sec:gw2v}, we use generalized Word2Vec approaches to find patterns in EHR data. We use the OSIM2 dataset and represent each EHR event as a $6$ dimensional vector. This vector is comparable to a word in a normal Word2Vec approach and functions as the abstract object in our generalized Word2Vec approaches. 

\subsection{Categorization of Features}
\label{sec:categorizationImpl}

Because we are working with high dimensional data and a each dimension has a wide range of possible values, most instances of OSIM2 are unique. To find patterns which are more generally applicable, we generalize our data by projecting values for some attributes into categories. See table \ref{tab:catOsim}. \\

\begin{table}[!htb]
\centering
\label{tab:catOsim}
\begin{tabular}{lll}
\hline
Attribute                                    & Requirement                      & Category \\ \hline
\multicolumn{1}{l|}{Time between EHR events} & \multicolumn{1}{l|}{$\leq 0$}    & 0        \\
\multicolumn{1}{l|}{}                        & \multicolumn{1}{l|}{$\leq 10$}   & 1        \\
\multicolumn{1}{l|}{}                        & \multicolumn{1}{l|}{$\leq 20$}   & 2        \\
\multicolumn{1}{l|}{}                        & \multicolumn{1}{l|}{$20 <$}      & 3        \\ \hline
\multicolumn{1}{l|}{Birthyear}               & \multicolumn{1}{l|}{$\leq 1900$} & 0        \\
\multicolumn{1}{l|}{}                        & \multicolumn{1}{l|}{$\leq 1910$} & 1        \\
\multicolumn{1}{l|}{}                        & \multicolumn{1}{l|}{$\leq 1920$} & 2        \\
\multicolumn{1}{l|}{}                        & \multicolumn{1}{l|}{\vdots}      & \vdots   \\
\multicolumn{1}{l|}{}                        & \multicolumn{1}{l|}{$\leq 2020$} & 12       \\ \hline
\multicolumn{1}{l|}{Season}                  & \multicolumn{1}{l|}{}            & 3 (winter) \\
\multicolumn{1}{l|}{}                        & \multicolumn{1}{l|}{}            & 4 (spring)   \\
\multicolumn{1}{l|}{}                        & \multicolumn{1}{l|}{}            & 1 (summer)   \\
\multicolumn{1}{l|}{}                        & \multicolumn{1}{l|}{}            & 2 (fall)    \\ \hline
\end{tabular}

\caption{Different categories for OSIM2 attributes.}
\end{table}

It is easy to generalize concepts such as time intervals and demographics, for example we can say that people between the age $0$ and $10$ belong to category A. This becomes complex for disease diagnoses as it requires a lot of knowledge to categorize those. We come back to disease coding in section \ref{sec:mapping}. \\

To see the effect of our categorizations, see table \ref{tab:general}. You can see the $3$ most common vectors from our dataset before and after the categorizations. The vectors have the following structure: \textit{{[}birthyear, gender, conditionType, diagnose, timeDifference, season{]}}. \\

\begin{table}[!htb]
\centering

\label{tab:general}
\begin{tabular}{lll}
\hline
\textbf{Before Categorization}                                                    &                             &                     \\ \hline
\textit{Vector} & \textit{Occurrences}        & \textit{Percentage} \\ \hline
\multicolumn{1}{l|}{{[}1956.0, 8532.0, 65.0, 5.00000701E8, 0.0, 3.0{]}}           & \multicolumn{1}{l|}{17574}  & 0.0095              \\
\multicolumn{1}{l|}{{[}1954.0, 8532.0, 65.0, 5.00000701E8, 0.0, 3.0{]}}           & \multicolumn{1}{l|}{17536}  & 0.0094              \\
\multicolumn{1}{l|}{{[}1955.0, 8532.0, 65.0, 5.00000701E8, 0.0, 3.0{]}}           & \multicolumn{1}{l|}{17476}  & 0.0094              \\
                                                                                  &                             &                     \\ \hline
\textbf{After Categorization}                                                     &                             &                     \\ \hline
\textit{Vector} & \textit{Occurrences}        & \textit{Percentage} \\ \hline
\multicolumn{1}{l|}{{[}6.0, 8532.0, 65.0, 784955.0, 1.0, 3.0{]}}                  & \multicolumn{1}{l|}{282086} & 0.15                \\
\multicolumn{1}{l|}{{[}7.0, 8532.0, 65.0, 784955.0, 1.0, 3.0{]}}                  & \multicolumn{1}{l|}{235459} & 0.13                \\
\multicolumn{1}{l|}{{[}5.0, 8532.0, 65.0, 784955.0, 1.0, 3.0{]}}                  & \multicolumn{1}{l|}{230216} & 0.12               
\end{tabular}

\caption{Three most common vectors in our dataset before and after categorization.}
\end{table}


\subsection{Disease Code Categorization and Mapping}
\label{sec:mapping}

In the section we discuss the method to categorize the disease codes in the OSIM2 dataset.  For example, a bruise on your right leg and a bruise on your left leg should both be categorized under a bruises category. \\

The OSIM2 dataset uses MedDRA disease codes for the diagnoses, see section \ref{sec:diseaseCodes}. We mentioned that MedDRA does not have an easy to use hierarchy. With a hierarchy, it would be trivial to categorize a disease code to its highest hierarchy. \\
To solve this, we map MedDRA disease codes to ICD-10 disease codes. The reasoning behind this is that ICD-10 provides a clear and easy to use hierarchy. Next to this, ICD-10 disease codes also make it possible to compare our results with \cite{Brunak:article}. \\
We would have liked to use the method described in \cite{icd10Mapping:article} to map the codes but therefore the disease mapping made by UMLS between ICD-10 and MedDRA is needed. This however is not publicly available. \\

The mapping is based on the description of each disease code. Both MedDRA and ICD-10 have a short medical description of each code. Each description is first filtered from stop words. Afterwards, the MedDRA code is matched to the ICD-10 code based on the matching words in both descriptions. The matching is defined as the ICD-10 code with the highest percentage of words matching. \\

In figure \ref{fig:mappingStats}, we show the statistics of this mapping process. Note that we only take disease codes into account if they are part of the OSIM2 dataset. In the figure, each bar represents the percentage of the words matching between descriptions. The height of the bars represent the percentage of all disease codes in the OSIM2 dataset which have this amount of word matches. \\

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.7\textwidth]{mappingStats.jpeg}
	\caption{Percentage of OSIM codes compared to their fraction of their matching words.}
	\label{fig:mappingStats}
\end{figure}

We find a $63$\% average of the words in the description that matche between MedDRA and ICD-10 descriptions. We also see that around $85$\% of all the disease code mappings have a match of at least $33$\%. We assume this is a reasonable mapping as medical terms are quite specific and if $33$\% matches, this corresponds to a match on to a higher hierarchical level. \\
For the codes which have a zero percent match, the Damerau-Levenshtein algorithm \cite{edit:article} is applied. The algorithm calculates the edit distance between two strings using character insertion, character deletion, character replacement, and adjacent character swaps. The description with the lowest edit distance is then chosen as a match. \\
Another option would have been to remove those codes from our dataset. As an EHR contains more information than only the disease code, a lot of information would have been lost. This however is not tested and is a possibility for future work. \\

With the mapping, we can now categorize the disease codes. We use the hierarchy of ICD-10 and reduce the ICD-10 code to its first $3$ symbols. For example, we reduce the code \textit{E00.1} to its higher category \textit{E00}.


\subsection{Comparison with Danish Clusters}
\label{sec:clusterExp}

To test our approaches we will compare our results with the clusters found from Anders Boeck Jensen et. \cite{Brunak:article}. For this, we also need to generate our own found clusters. This method is described in this section. Note that the comparison with the Danish paper is not ideal but it is currently the largest study performed on EHRs. We refer to chapter \ref{cha:futureWork} for more information about another possible experiment in the future. \\

First we apply our approaches on the OSIM dataset. For a description of the used parameters and their functions, see section \ref{sec:parameters}. As end result, we get a lookup table containing the mapping between the old vector space and the new one. From this lookup table, we take EHRs and find their k-nearest neighbors depending on their mapping in the new vector space (see section \ref{sec:parameters} for more information about clusterK). Depending on the approach, we take certain EHRs. For the generalized Word2Vec, we check all EHRs in the lookup table. For the knn Word2Vec, we only check the complete new instances added to the lookup table using the knn method. For DeepWalk, we again take all EHRs in the lookup table. \\

Based on the formed clusters from our lookup table (Word2Vec clusters), we can now compare it to the Danish clusters. We test the matching between the two clusters based on two experiments. \\
The first one is as follows (we later call this experiment $1$):

\begin{itemize}

\item We start with an element $e_{wv}$ from the lookup table and form a Word2Vec cluster $C_{wv}$ around it.
\item We then check to which Danish cluster $C_d$ element $e_{wv}$ belongs.
\item For each element in $C_{wv}$, we check if it belongs to $C_d$.
\begin{itemize}
\item If it does, we add $1$ to the number $t_{matches}$.
\item If it does not, we add $1$ to the number $t_{non\_matches}$.
\end{itemize}
\item After doing this for all elements in $C_{wv}$, we calculate the match percentage as $p_{match} = \frac{t\_{matches}}{t_{matches} + t_{non\_matches}}$.

\end{itemize}

\noindent The second one is as follows (we later call this experiment $2$):

\begin{itemize}

\item We start with an element $e_{wv}$ from the lookup table and form a Word2Vec cluster $C_{wv}$ around it.
\item We then check to which Danish cluster $C_d$ element $e_{wv}$ belongs.
\item We set $t_{d}$ equals to the amount of elements in $C_d$.
\item For each element in $C_{wv}$, we check if it belongs to $C_d$.
\begin{itemize}
\item If it does, we add $1$ to the number $t_{matches}$.
\end{itemize}
\item After doing this for all elements in $C_{wv}$, we calculate the match percentage as $p_{match} = \frac{t\_{matches}}{t_{d}}$.

\end{itemize}

For each method, we calculate the average $p_{match}$ over all elements of the lookup table and keep track of the maximum and minimum value for $p_{match}$.


\subsection{Parameters}
\label{sec:parameters}

The effectiveness of a neural network like the one used to learn the disease embeddings, depends on parameter tuning \cite{tuning:article}. Therefore we explored some different settings. Note that this is not an extensive parameter tuning setup, but an overview on the effect and logic behind some parameters. An extensive parameter tuning setup is possible as future work. \\

In table \ref{tab:parameters}, you can see an overview of the different parameters for each approach. For each approach, all possible combinations of the mentioned parameters have been tested. \\

\begin{table}[!htb]
\centering

\label{tab:parameters}
\begin{tabular}{llll}
\hline
Parameter                                    & Generalized Word2Vec  & Knn Word2Vec          & DeepWalk              \\ \hline
\multicolumn{1}{l|}{Vectorlength}            & {[}50, 100{]}         & {[}50, 100{]}         & {[}50, 100{]}         \\
\multicolumn{1}{l|}{Batch Size}              & 500                   & 500                   & 500                   \\
\multicolumn{1}{l|}{Epoch}                   & 1                     & 1                     & 1                     \\
\multicolumn{1}{l|}{Window Size}             & {[}5, 10, 15{]}       & {[}5, 10, 15{]}       & {[}5, 10, 15{]}       \\
\multicolumn{1}{l|}{Learning Rate}           & {[}0.025, 0.1{]}      & {[}0.025, 0.1{]}      & {[}0.025, 0.1{]}      \\
\multicolumn{1}{l|}{Minimum Word Frequency} & {[}5, 10{]}           & {[}5, 10{]}           & {[}5, 10{]}           \\
\multicolumn{1}{l|}{ClusterK}                & {[}100, 1000, 5000{]} & {[}100, 1000, 5000{]} & {[}100, 1000, 5000{]} \\
\multicolumn{1}{l|}{K}                       & /                     & {[}10, 50, 100{]}     & /                     \\
\multicolumn{1}{l|}{Walklength}              & /                     & /                     & {[}5, 10, 15{]}      
\end{tabular}

\caption{Parameters which are tested for the different approaches}
\end{table}

\noindent\textbf{Vectorlength} decides the dimensions of the new vector space to which the diseases are projected to. A larger size is more expressive but a smaller size decreases the complexity of the vocabulary after the training. Each dimension can be seen as a representation of an unknown linear relation to some context-disease pairs \cite{vl:article}. \\
\textbf{Batch size} is the number of training examples used in one gradient descent step (both forward and backward pass). \\
\textbf{Epoch} is the amount of times you go over all the training examples and use them to train your neural network. \\
\textbf{Window size} is the size of the n-gram we use to create contexts. A larger window tends to make more specific contexts and therefore get a better model \cite{w2vOriginal:article} \cite{windowSize:article}. \\
\textbf{Learning rate} decides how the weights are adjusted during backpropagation. A large value can make the neural network learn faster but may also cause the network to not learn at all. A small value on the other hand can cause a slow convergence or overfitting \cite{lr:article}. Note that in our implementation there is a decay of the learning rate as the number of examples left decreases. \\
\textbf{Minimum Word Frequency} removes instances from the training set below the minimum. A low frequency causes some instances to only have a few contexts where relations can be learned from. So a higher frequency could improve the accuracy for a Word2Vec model but also throw away information. Note that the influence of this parameter is not well researched. \\
\textbf{ClusterK} is the parameter which creates the Word2Vecclusters as described in the previous section. A higher clusterK causes the Word2Vec cluster to be larger and a lower value the other way around. \\
\textbf{K} is a specific parameter for the knn Word2Vec approach. It decides on how many nearest neighbors the new vector representation is based on. For more explanation see chapter \ref{cha:background}. \\
\textbf{Walklength} is a specific parameter for DeepWalk. It decides how long the generated sequence will be. For more explanation see chapter \ref{cha:background}. \\


\subsection{Generalized Word2Vec}

Here we discuss the results from our two experiments on the generalized Word2Vec approach. We trained a Word2Vec model on the complete OSIM2 dataset. From this model, we compare the Word2Vec clusters with the Danish cluster as described in section \ref{sec:clusterExp}. For each parameter, we fix the other parameters and check their influence on the matching percentage. After checking the individual parameters, we take the parameters setting with the best matching percentage and interpret those results.

\subsubsection*{Vectorlength}

Both in experiment $1$ and experiment $2$, we found that the vectorlength had no substantial influence on the matching percentage. See figure \ref{fig:s2v_vl_1} for the matching percentage of experiment $1$ and figure \ref{fig:s2v_vl_2} for experiment $2$. The two bars in the figures each show a parameter value and the black stripes both represent the minimum and the maximum value of the matching percentage. Similar results are achieved with all the other parameter settings. \\

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.6\textwidth]{S2V1_vl.jpeg}
	\caption{Mapping percentage with the vectorlength parameter of generalized Word2Vec approach in experiment 1.}
	\label{fig:s2v_vl_1}
\end{figure}

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.6\textwidth]{S2V2_vl.jpeg}
	\caption{Mapping percentage with the vectorlength parameter of generalized Word2Vec approach in experiment 2.}
	\label{fig:s2v_vl_2}
\end{figure}

We conclude that a vectorlength of $50$ is already expressive enough to differentiate between different diseases. Only sometimes the vectorlength of $100$ gives a small gain in matching percentage which can be explained by the increase of expressiveness of the new vector space.

\subsubsection*{Window Size}

Both in experiment $1$ and experiment $2$, we found that the window size had no substantial influence on the matching percentage. See figure \ref{fig:s2v_vl_1} for the matching percentage of experiment $1$ and figure \ref{fig:s2v_vl_2} for experiment $2$. Similar results are achieved with all the other parameter settings. \\

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.6\textwidth]{S2V1_ws.jpeg}
	\caption{Mapping percentage with the window size parameter of generalized Word2Vec approach in experiment 1.}
	\label{fig:s2v_vl_1}
\end{figure}

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.6\textwidth]{S2V2_ws.jpeg}
	\caption{Mapping percentage with the window size parameter of generalized Word2Vec approach in experiment 2.}
	\label{fig:s2v_vl_2}
\end{figure}

We expected the window size to have more impact as a higher size makes more specific contexts. And more specific contexts, tend to make better contexts. A reason for this could be that we don't have enough data to justify more specific contexts. Also, because our sequence lengths have an average length of around $19$, a window size of $5$ is not that much different than a size of $10$ as in many cases it will cut of the size at the start or end of the sequence. The Danish paper uses sequences of length $4$, so this could also imply that a window size of $5$ is already enough to cover the relationships the Danish paper found and therefore a higher size will not have an impact on the matching percentage. 

\subsubsection*{Learning Rate}

Both in experiment $1$ and experiment $2$, we found that the learning rate had no substantial influence on the matching percentage. See figure \ref{fig:s2v_vl_1} for the matching percentage of experiment $1$ and figure \ref{fig:s2v_vl_2} for experiment $2$. Similar results are achieved with all the other parameter settings. \\

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.6\textwidth]{S2V1_lr.jpeg}
	\caption{Mapping percentage with the learning rate parameter of generalized Word2Vec approach in experiment 1.}
	\label{fig:s2v_vl_1}
\end{figure}

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.6\textwidth]{S2V2_lr.jpeg}
	\caption{Mapping percentage with the learning rate parameter of generalized Word2Vec approach in experiment 2.}
	\label{fig:s2v_vl_2}
\end{figure}

Learning rate is a parameter which is hard to tune. This explains the amount of literature around learning rate and the implementation of automatic tuning algorithms for learning rate. Therefore our results do not provide any possible conclusion as we only tested $2$ different learning rates. \\
Although it is also possible that we chose our learning rate high enough in both cases and because of the learning decay, they both achieved the same results.

\subsubsection*{Minimum Word Frequency}

Both in experiment $1$ and experiment $2$, we found that the minimum word frequency had no substantial influence on the matching percentage. See figure \ref{fig:s2v_vl_1} for the matching percentage of experiment $1$ and figure \ref{fig:s2v_vl_2} for experiment $2$. Similar results are achieved with all the other parameter settings. \\

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.6\textwidth]{S2V1_wf.jpeg}
	\caption{Mapping percentage with the minimum word frequency parameter of generalized Word2Vec approach in experiment 1.}
	\label{fig:s2v_vl_1}
\end{figure}

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.6\textwidth]{S2V2_wf.jpeg}
	\caption{Mapping percentage with the minimum word frequency parameter of generalized Word2Vec approach in experiment 2.}
	\label{fig:s2v_vl_2}
\end{figure}

We expected the minimum word frequency to have a better accuracy with a higher value as we would be sure they have enough instances to be learned from. Because we applied generalization, $99.82$\% of the instances are above the minimum word frequency of $10$ to $69.90$\% before generalization. We therefore expect the minimum word frequency to have lost its influence. A future direction to explore is to ignore the generalization and solely work with minimum word frequency.

\subsubsection*{ClusterK}

See figure \ref{fig:s2v_vl_1} for the influence in the matching percentage of experiment $1$ by the parameter clusterK. Similar results are achieved with all the other parameter settings. \\

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.6\textwidth]{S2V1_clusterK.jpeg}
	\caption{Mapping percentage with the clusterK parameter of generalized Word2Vec approach in experiment 1.}
	\label{fig:s2v_vl_1}
\end{figure}

\noindent We see a trend that with a lower clusterK there is a higher matching percentage. This means that in the smaller clusters, there is a higher density of instances which correspond to the Danish clusters. This shows that our method is not completely random especially in the case of some disease codes as a maximum matching percentage of $55$\% is found. \\

\noindent See figure \ref{fig:s2v_vl_1} for the influence in the matching percentage of experiment $2$ by the parameter clusterK. Similar results are achieved with all the other parameter settings. \\

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.6\textwidth]{S2V2_clusterK.jpeg}
	\caption{Mapping percentage with the clusterK parameter of generalized Word2Vec approach in experiment 2.}
	\label{fig:s2v_vl_1}
\end{figure}

\noindent We see that most matches with the Danish clusters are found in the smaller Word2Vec clusters. This corresponds with the results from experiment $1$.

\subsection{K-Nearest Neighbors Word2Vec}

\subsubsection*{Vectorlength}

Similar as generalized word2vec

\subsubsection*{Window Size}

Similar as generalized word2vec

\subsubsection*{Learning Rate}

Similar as generalized word2vec

\subsubsection*{Minimum Word Frequency}

Similar as generalized word2vec

\subsubsection*{ClusterK}

\subsubsection*{K}

TODO

\subsection{DeepWalk}

TODO

\subsubsection*{Vectorlength}

Similar as generalized word2vec

\subsubsection*{Window Size}

Similar as generalized word2vec

\subsubsection*{Learning Rate}

Similar as generalized word2vec

\subsubsection*{Minimum Word Frequency}

Similar as generalized word2vec

\subsubsection*{ClusterK}

TODO

\subsubsection*{Walklength}

Brunak 4


\subsection{Model Comparison}

TODO

\section{Conclusion}





%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
